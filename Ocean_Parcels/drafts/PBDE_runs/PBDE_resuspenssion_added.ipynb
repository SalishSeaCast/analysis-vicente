{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Script is for Running longer simulations of a standar PBDE set up with their different States using probabilities without POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import random\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "from parcels import FieldSet, Field, VectorField, ParticleSet, JITParticle, ParcelsRandom, Variable, Kernel, AdvectionRK4\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from cartopy import crs, feature\n",
    "import zarr \n",
    "\n",
    "sys.path.append('/ocean/vvalenzuela/MOAD/Ocean_Parcels')\n",
    "\n",
    "from OP_functions import *\n",
    "import cmocean\n",
    "cmap = cmocean.cm.deep\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_WW3_path(date):\n",
    "    \"\"\"Construct WW3 results path given the date\n",
    "    e.g., /opp/wwatch3/nowcast/SoG_ww3_fields_YYYYMMDD_YYYYMMDD.nc\n",
    "    :arg date: date of WW3 record\n",
    "    :type date: :py:class:`datetime.datetime`\n",
    "    :returns: WW3 path\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    # Make WW3 path\n",
    "    path = '/opp/wwatch3/hindcast'\n",
    "    path2 = '/opp/wwatch3/nowcast'\n",
    "    datestr = [date.strftime(fmt) for fmt in ('%d%b%y', '%Y%m%d_%Y%m%d')]\n",
    "    path = os.path.join(path, datestr[0].lower(), f'SoG_ww3_fields_{datestr[1]}.nc')\n",
    "    if not os.path.exists(path):\n",
    "        path = os.path.join(path2, datestr[0].lower(), f'SoG_ww3_fields_{datestr[1]}.nc')\n",
    "        if not os.path.exists(path):    \n",
    "            raise ValueError(f\"No WW3 record found for the specified date {date.strftime('%Y-%b-%d')}\")\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = {'NEMO': '/results2/SalishSea/nowcast-green.202111/',\n",
    "'coords': '/ocean/vvalenzuela/MOAD/grid/coordinates_seagrid_SalishSea201702.nc',\n",
    "'coordsWW3': '/ocean/vvalenzuela/MOAD/grid2/WW3_grid.nc',\n",
    "'mask': '/ocean/vvalenzuela/MOAD/grid2/mesh_mask202108_TDV.nc',\n",
    "'bat': '/ocean/vvalenzuela/MOAD/grid/bathymetry_202108.nc',\n",
    "'out': '/home/vvalenzuela/MOAD/Ocean_Parcels/results/PBDE_runs',\n",
    "'home': '/home/vvalenzuela/MOAD/Ocean_Parcels',\n",
    "'anim': '/home/vvalenzuela/MOAD/Ocean_Parcels/results/PBDE_runs/animations'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = xr.open_dataset(path['coords'], decode_times=False)\n",
    "mask = xr.open_dataset(path['mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamps(start,length):\n",
    "    timestamps=[]\n",
    "    duration = timedelta(days=length)\n",
    "    for day in range(duration.days):\n",
    "        timestamps.append([start + timedelta(days=day)])\n",
    "    return np.array(timestamps, dtype='datetime64')\n",
    "\n",
    "def find_temp(rootdir):\n",
    "    dirs=[]\n",
    "    for file in os.listdir(rootdir):\n",
    "        d = os.path.join(rootdir, file)\n",
    "        if os.path.isdir(d):\n",
    "            dirs.append(d)\n",
    "    temp=sorted(dirs, key=lambda x: os.path.getctime(x), reverse=True)[:1][0]\n",
    "    return temp[-12:]\n",
    "\n",
    "def newest(path):\n",
    "    files = os.listdir(path)\n",
    "    paths = [os.path.join(path, basename) for basename in files]\n",
    "    return max(paths, key=os.path.getctime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definitions\n",
    "start = datetime(2022, 1, 1) #Start date\n",
    "length = 30 # Set Time length [days] \n",
    "dt = 90 #toggle between - or + to pick backwards or forwards \n",
    "N = 1 # 1000   # number of  locations\n",
    "# Here the number of particles needs to be associated to the discharge rates from the Iona Outfall\n",
    "n = 2000 # number of particles per location\n",
    "dmin = 60#minimum depth\n",
    "dd = 20 #max depth difference from dmin\n",
    "dtp = 0\n",
    "odt = 1 #mins\n",
    "rrr = 1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get grid point gridX and gridY\n",
    "path_NEMO = make_prefix(datetime(2022, 1, 1),path['NEMO'])\n",
    "jjii = xr.open_dataset('/ocean/vvalenzuela/MOAD/grid/grid_from_lat_lon_mask999.nc')\n",
    "def finder(lati,loni):\n",
    "    j = [jjii.jj.sel(lats=lati, lons=loni, method='nearest').item()][0]\n",
    "    i = [jjii.ii.sel(lats=lati, lons=loni, method='nearest').item()][0]\n",
    "    return j,i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total depth at this location is 85.375 m\n"
     ]
    }
   ],
   "source": [
    "#Set deploy coordinates following yaml   \n",
    "clat = [49.195045]\n",
    "clon = [-123.301956]\n",
    "#\n",
    "a, b = finder(clat[0], clon[0])\n",
    "print (\"The total depth at this location is\", mask.totaldepth[a, b].values, 'm')\n",
    "#\n",
    "duration = timedelta(days=length)\n",
    "#\n",
    "x_offset, y_offset, z = p_deploy(N,n,dmin,dd,rrr)\n",
    "#\n",
    "lon = np.zeros([N,n])\n",
    "lat = np.zeros([N,n])\n",
    "for i in range(N):\n",
    "    lon[i,:]=(clon[i] + x_offset[i,:])\n",
    "    lat[i,:]=(clat[i] + y_offset[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files for states runs\n",
    "name_states = 'Resuspenssion_run_PBDEs_'\n",
    "daterange = [start+timedelta(days=i) for i in range(length)]\n",
    "fn =  name_states + '_'.join(d.strftime('%Y%m%d')+'_1n' for d in [start, start+duration]) + '.zarr'\n",
    "outfile_states = os.path.join(path['out'], fn)\n",
    "#\n",
    "local = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vvalenzuela/conda_envs/Parcels/lib/python3.11/site-packages/parcels/field.py:511: FileWarning: File /ocean/vvalenzuela/MOAD/grid/coordinates_seagrid_SalishSea201702.nc could not be decoded properly by xarray (version 2024.3.0). It will be opened with no decoding. Filling values might be wrongly parsed.\n",
      "  with _grid_fb_class(lonlat_filename, dimensions, indices, netcdf_engine) as filebuffer:\n",
      "/home/vvalenzuela/conda_envs/Parcels/lib/python3.11/site-packages/parcels/field.py:511: FileWarning: File /ocean/vvalenzuela/MOAD/grid/coordinates_seagrid_SalishSea201702.nc could not be decoded properly by xarray (version 2024.3.0). It will be opened with no decoding. Filling values might be wrongly parsed.\n",
      "  with _grid_fb_class(lonlat_filename, dimensions, indices, netcdf_engine) as filebuffer:\n",
      "/home/vvalenzuela/conda_envs/Parcels/lib/python3.11/site-packages/parcels/field.py:511: FileWarning: File /ocean/vvalenzuela/MOAD/grid/coordinates_seagrid_SalishSea201702.nc could not be decoded properly by xarray (version 2024.3.0). It will be opened with no decoding. Filling values might be wrongly parsed.\n",
      "  with _grid_fb_class(lonlat_filename, dimensions, indices, netcdf_engine) as filebuffer:\n",
      "/home/vvalenzuela/conda_envs/Parcels/lib/python3.11/site-packages/parcels/field.py:511: FileWarning: File /ocean/vvalenzuela/MOAD/grid/coordinates_seagrid_SalishSea201702.nc could not be decoded properly by xarray (version 2024.3.0). It will be opened with no decoding. Filling values might be wrongly parsed.\n",
      "  with _grid_fb_class(lonlat_filename, dimensions, indices, netcdf_engine) as filebuffer:\n",
      "/home/vvalenzuela/conda_envs/Parcels/lib/python3.11/site-packages/parcels/field.py:511: FileWarning: File /ocean/vvalenzuela/MOAD/grid/coordinates_seagrid_SalishSea201702.nc could not be decoded properly by xarray (version 2024.3.0). It will be opened with no decoding. Filling values might be wrongly parsed.\n",
      "  with _grid_fb_class(lonlat_filename, dimensions, indices, netcdf_engine) as filebuffer:\n",
      "/home/vvalenzuela/conda_envs/Parcels/lib/python3.11/site-packages/parcels/field.py:511: FileWarning: File /ocean/vvalenzuela/MOAD/grid/coordinates_seagrid_SalishSea201702.nc could not be decoded properly by xarray (version 2024.3.0). It will be opened with no decoding. Filling values might be wrongly parsed.\n",
      "  with _grid_fb_class(lonlat_filename, dimensions, indices, netcdf_engine) as filebuffer:\n",
      "/home/vvalenzuela/conda_envs/Parcels/lib/python3.11/site-packages/parcels/field.py:511: FileWarning: File /ocean/vvalenzuela/MOAD/grid/coordinates_seagrid_SalishSea201702.nc could not be decoded properly by xarray (version 2024.3.0). It will be opened with no decoding. Filling values might be wrongly parsed.\n",
      "  with _grid_fb_class(lonlat_filename, dimensions, indices, netcdf_engine) as filebuffer:\n",
      "/home/vvalenzuela/conda_envs/Parcels/lib/python3.11/site-packages/parcels/field.py:511: FileWarning: File /ocean/vvalenzuela/MOAD/grid/coordinates_seagrid_SalishSea201702.nc could not be decoded properly by xarray (version 2024.3.0). It will be opened with no decoding. Filling values might be wrongly parsed.\n",
      "  with _grid_fb_class(lonlat_filename, dimensions, indices, netcdf_engine) as filebuffer:\n",
      "/home/vvalenzuela/conda_envs/Parcels/lib/python3.11/site-packages/parcels/field.py:511: FileWarning: File /ocean/vvalenzuela/MOAD/grid/coordinates_seagrid_SalishSea201702.nc could not be decoded properly by xarray (version 2024.3.0). It will be opened with no decoding. Filling values might be wrongly parsed.\n",
      "  with _grid_fb_class(lonlat_filename, dimensions, indices, netcdf_engine) as filebuffer:\n",
      "/home/vvalenzuela/conda_envs/Parcels/lib/python3.11/site-packages/parcels/field.py:511: FileWarning: File /ocean/vvalenzuela/MOAD/grid/coordinates_seagrid_SalishSea201702.nc could not be decoded properly by xarray (version 2024.3.0). It will be opened with no decoding. Filling values might be wrongly parsed.\n",
      "  with _grid_fb_class(lonlat_filename, dimensions, indices, netcdf_engine) as filebuffer:\n"
     ]
    }
   ],
   "source": [
    "varlist=['U','V','W']\n",
    "filenames,variables=filename_set(start,length,varlist)\n",
    "dimensions = {'lon': 'glamf', 'lat': 'gphif', 'depth': 'depthw','time': 'time_counter'}\n",
    "field_set=FieldSet.from_nemo(filenames, variables, dimensions, allow_time_extrapolation=True, chunksize='auto')\n",
    "\n",
    "#Find file names and variable names ###'Diat','Flag'###\n",
    "varlist=['US','VS','WL','R','T','S','ssh','Bathy','Kz','totdepth','Vol']\n",
    "filenames,variables=filename_set(start,length,varlist)\n",
    "dimensions = {'lon': 'glamt', 'lat': 'gphit', 'depth': 'deptht', 'time': 'time_counter'}\n",
    "density = Field.from_netcdf(filenames['R'], variables['R'], dimensions,allow_time_extrapolation=True, chunksize='auto')\n",
    "field_set.add_field(density)\n",
    "#\n",
    "#Add Vertical diffusivity coefficient field\n",
    "dimensions = {'lon': 'glamt', 'lat': 'gphit', 'depth': 'depthw','time': 'time_counter'}\n",
    "Kz = Field.from_netcdf(filenames['Kz'], variables['Kz'], dimensions,allow_time_extrapolation=True, chunksize='auto')\n",
    "field_set.add_field(Kz)\n",
    "#\n",
    "#Add Bathymetry 2D field\n",
    "dimensions = {'lon': 'glamt', 'lat': 'gphit'}\n",
    "Bth = Field.from_netcdf(filenames['Bathy'], variables['Bathy'], dimensions,allow_time_extrapolation=True, chunksize='auto')\n",
    "TD = Field.from_netcdf(filenames['totdepth'], variables['totdepth'], dimensions,allow_time_extrapolation=True, chunksize='auto')\n",
    "field_set.add_field(Bth)\n",
    "field_set.add_field(TD)\n",
    "#\n",
    "#Add SSH \n",
    "dimensions = {'lon': 'glamt', 'lat': 'gphit','time': 'time_counter'}\n",
    "SSH = Field.from_netcdf(filenames['ssh'], variables['ssh'], dimensions,allow_time_extrapolation=True, chunksize='auto')\n",
    "field_set.add_field(SSH)\n",
    "#\n",
    "# Add e3t\n",
    "varlist = ['cell_size']\n",
    "filenames,variables=filename_set(start,length,varlist)\n",
    "dimensions = {'lon': 'glamt', 'lat': 'gphit', 'depth': 'deptht', 'time': 'time_counter'}\n",
    "E3T = Field.from_netcdf(filenames['cell_size'], variables['cell_size'], dimensions,allow_time_extrapolation=True, chunksize='auto')\n",
    "field_set.add_field(E3T)\n",
    "#\n",
    "# Add mbathy\n",
    "varlist = ['last_cell_index']\n",
    "filenames,variables=filename_set(start,length,varlist)\n",
    "dimensions = {'lon': 'glamt', 'lat': 'gphit', 'time' : 't'}\n",
    "MBATHY = Field.from_netcdf(filenames['last_cell_index'], variables['last_cell_index'], dimensions,allow_time_extrapolation=True, chunksize='auto')\n",
    "field_set.add_field(MBATHY)\n",
    "####################################################### ADD IF USING POC or DOC###################################################################\n",
    "# Fieldest for PON \n",
    "#varlist=['PON', 'DIATO', 'FLAGE']\n",
    "#filenames, variables = filename_set(start, length, varlist)\n",
    "#dimensions = {'lon': 'glamf', 'lat': 'gphif', 'depth': 'deptht','time': 'time_counter'}\n",
    "#PON=Field.from_netcdf(filenames['PON'], variables['PON'], dimensions, allow_time_extrapolation=True, chunksize='auto')\n",
    "#DIATO=Field.from_netcdf(filenames['DIATO'], variables['DIATO'], dimensions, allow_time_extrapolation=True, chunksize='auto')\n",
    "#FLAGE=Field.from_netcdf(filenames['FLAGE'], variables['FLAGE'], dimensions, allow_time_extrapolation=True, chunksize='auto')\n",
    "#\n",
    "#field_set.add_field(PON)\n",
    "#field_set.add_field(DIATO)\n",
    "#field_set.add_field(FLAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Ocean Parcels doesn't like to have numpy libraries inside his Kernels, we need to create and add a $U^{*2}$ fieldset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3600"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_set.computeTimeChunk(time=0, dt=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 109.07 MiB </td>\n",
       "                        <td> 54.54 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (2, 40, 898, 398) </td>\n",
       "                        <td> (1, 40, 898, 398) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 2 chunks in 13 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float32 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"313\" height=\"189\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"25\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"0\" x2=\"12\" y2=\"25\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,25.412616514582485 0.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >2</text>\n",
       "  <text x=\"45.412617\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,45.412617,12.706308)\">1</text>\n",
       "\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"114\" y2=\"19\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"95\" y1=\"120\" x2=\"114\" y2=\"139\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"95\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"114\" y1=\"19\" x2=\"114\" y2=\"139\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"95.0,0.0 114.87811293346246,19.878112933462468 114.87811293346246,139.87811293346246 95.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"148\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"114\" y1=\"19\" x2=\"168\" y2=\"19\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"95\" y1=\"0\" x2=\"114\" y2=\"19\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"148\" y1=\"0\" x2=\"168\" y2=\"19\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"95.0,0.0 148.184855233853,0.0 168.06296816731546,19.878112933462468 114.87811293346246,19.878112933462468\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"114\" y1=\"19\" x2=\"168\" y2=\"19\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"114\" y1=\"139\" x2=\"168\" y2=\"139\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"114\" y1=\"19\" x2=\"114\" y2=\"139\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"168\" y1=\"19\" x2=\"168\" y2=\"139\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"114.87811293346246,19.878112933462468 168.06296816731546,19.878112933462468 168.06296816731546,139.87811293346246 114.87811293346246,139.87811293346246\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"141.470541\" y=\"159.878113\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >398</text>\n",
       "  <text x=\"188.062968\" y=\"79.878113\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,188.062968,79.878113)\">898</text>\n",
       "  <text x=\"94.939056\" y=\"149.939056\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,94.939056,149.939056)\">40</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<where, shape=(2, 40, 898, 398), dtype=float32, chunksize=(1, 40, 898, 398), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_set.U.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PARTICLE TYPE and variables within itself\n",
    "class MPParticle(JITParticle):    \n",
    "    n = Variable('n', initial = n)\n",
    "    vvl_factor = Variable('fact', initial =  1)    \n",
    "    wa = Variable('wa', initial =  0) \n",
    "    wm = Variable('wm', initial =  0)\n",
    "    initialized = Variable('initialized', initial = 0)\n",
    "    status = Variable('status', initial =  1) # different status for different processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pset_states = ParticleSet.from_list(field_set, MPParticle, lon=lon, lat=lat, depth=z,time=start+timedelta(hours=odt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textbf{Kernels!}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for the beginning, not at every time!!\n",
    "def states(particle, fieldset, time):\n",
    "    if particle.initialized == 0:\n",
    "        n = particle.n \n",
    "        # n is the total amount of particles released at the starting location\n",
    "        data = ParcelsRandom.randint(0, n-1)\n",
    "       # print('States Kernel is Running')\n",
    "\n",
    "        #\n",
    "        # PBDEs as Sewage Particles\n",
    "        if data < 3*(n/4):\n",
    "            particle.status = 1\n",
    "        #\n",
    "        # Colloidal/Dissolved PBDEs\n",
    "        else:\n",
    "            particle.status = 2\n",
    "        #\n",
    "        particle.initialized = 1    \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Sun et al. (2023), Johannssen et al. (2005) and among other cited papers there is a linear relation between more PBDEs in colloidal form and POC concentrations.\n",
    "\n",
    "Using this criteria we can change our random absorption and desorption values to the POC obtained by the Redfield Ratio from the Particulate Organic Nitrogen in the Salish Sea Cast Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check our equation again:\n",
    "\n",
    "\n",
    "From Sun et al. 2023 we obtain a partitioning relation of PBDEs between dissolved and particulate states:\n",
    "\n",
    "\n",
    "\n",
    "$K_{oc,a} = [PBDE]_p / ([PBDE]_{d,c} \\cdot [POC])$\n",
    "\n",
    "\n",
    "Assuming that $K_{oc,a}$ is a constant, then we can get the following relation for POC:\n",
    "\n",
    "$[POC] = \\frac{[PBDE]_p}{[PBDE]_{d,c}} \\cdot \\frac{1}{K_{oc,a}}$\n",
    "\n",
    "\n",
    "So, \n",
    "\n",
    "\n",
    "- IF $[POC]$ is HIGH $\\Rightarrow$ HIGH $[PBDE]_p$ is expected \n",
    "\n",
    "- IF $[POC]$ is LOW $\\Rightarrow$ HIGH $[PBDE]_{d,c}$ is expected \n",
    "\n",
    "\n",
    "Rewritting it, we can get:\n",
    "\n",
    "\n",
    "$[POC] \\cdot K_{oc,a} = \\frac{[PBDE]_p}{[PBDE]_{d,c}}$\n",
    "\n",
    "So, if we have our mean POC value from the Model, we can get an estimate of Particulate and Dissolved PBDEs as follows:\n",
    "\n",
    "- IF $[POC] \\cdot K_{oc,a}$ is HIGHER than $[POC]_{mean}$ $\\Rightarrow$ HIGH $[PBDE]_p$ is expected\n",
    "\n",
    "- IF $[POC] \\cdot K_{oc,a}$ is LOWER than $[POC]_{mean}$ $\\Rightarrow$ HIGH $[PBDE]_p$ is expected \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of mean POC values per region:\n",
    "\n",
    "\n",
    "|| Northern Strait of Georgia | Central Strait of Georgia | Sothern Strait of Georgia |\n",
    "|--------------------------| -------------------------- | ------------------------- | ------------------------- |\n",
    "| $\\textbf{POC}$ $[mmol m^{-3}]$ | $\\textbf{0.04270812763156844}$ |  $\\textbf{0.04049420431621519}$ |  $\\textbf{0.0454328345755736}$ |\n",
    "| $\\textbf{POC}$ $[Kg / L]$ | $\\textbf{0.0005129545085444791}$ |  $\\textbf{0.00048636373978076585}$ |  $\\textbf{0.0005456801462368419}$ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(particle, fieldset, time):\n",
    "# After PBDEs are out from the pipe, they can absorbe into marine particles. \n",
    "# And , from marine particles they can desorbe into colloidal/dissolved PBDEs\n",
    "    abso = 0.038/24 #0.3\n",
    "    deso_s = 3.2/24 #0.2\n",
    "    deso_m = 1.6/24 #0.1\n",
    "    dt = 1\n",
    "    #print('Process Kernel is Running')\n",
    "\n",
    "    # If POC is above the mean concentration in the water column, then we are expecting more PBDEs in Colloidal form. \n",
    "    # But, if POC is below the mean, then we can expect more PBDEs in particulate form (Sewage and Marine Particles)\n",
    "    # Desorption from sewage particle to marine particle\n",
    "    if particle.initialized == 1:\n",
    "        value = ParcelsRandom.random()\n",
    "        if particle.status == 1 and value < (deso_s * dt):\n",
    "            particle.status = 2\n",
    "            # From Sewage Particle to Colloidal/Dissolved PBDE form\n",
    "            # POC * deso_s * dt < mean POC ----> PBDEs in dissolved form are expected\n",
    "        elif particle.status == 2 and value < (abso * dt):\n",
    "            particle.status = 3\n",
    "            # From Coloidal/Dissolved form to being attached to a Marine Particle           \n",
    "            # POC * abso * dt < mean POC ----> PBDEs in dissolved form are expected\n",
    "        elif particle.status == 3 and value < (deso_m * dt):\n",
    "            particle.status = 2\n",
    "            # From a Marine Particle to a Colloidal/Dissolved form          \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PBDEs_forms(particle, fieldset, time):\n",
    "    #print('PBDEs_forms Kernel is Running')\n",
    "    #### Lets say that the sinking rate of Sewage Particles is 3 times faster than the one for Marine Particles ####\n",
    "    # Sinking velocity for PBDEs in sewage particles\n",
    "    if particle.status == 1:\n",
    "        sinkvel = 1\n",
    "        particle.depth += sinkvel * particle.dt\n",
    "    # Sinking velocity when colloids (just float around)        \n",
    "    elif particle.status == 2:\n",
    "        sinkvel = 0.0\n",
    "        particle.depth += sinkvel * particle.dt\n",
    "    # Sinking velocity for PBDEs in marine particles\n",
    "    elif particle.status == 3:\n",
    "        sinkvel = 0.36\n",
    "        particle.depth += sinkvel * particle.dt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Advection(particle, fieldset, time): \n",
    "    #print('Advection Kernel is Running')\n",
    "    #\n",
    "    if particle.status == 1 or particle.status == 2 or particle.status == 3:\n",
    "        ssh = fieldset.sossheig[time, particle.depth, particle.lat, particle.lon] #SSH(t) sea surface height\n",
    "        sshn = fieldset.sossheig[time+particle.dt, particle.depth, particle.lat, particle.lon] #SSH(t+dt) sea surface height in the next time step\n",
    "        td = fieldset.totaldepth[time, particle.depth, particle.lat, particle.lon]#Total_depth \n",
    "        particle.fact = (1+ssh/td)\n",
    "        VVL = (sshn-ssh)*particle.depth/(td+ssh)\n",
    "        (u1, v1, w1) = fieldset.UVW[time, particle.depth, particle.lat, particle.lon]\n",
    "        lon1 = particle.lon + u1*.5*particle.dt\n",
    "        lat1 = particle.lat + v1*.5*particle.dt\n",
    "        dep1 = particle.depth + w1*.5*particle.dt/particle.fact\n",
    "        (u2, v2, w2) = fieldset.UVW[time + .5 * particle.dt, dep1, lat1, lon1]\n",
    "        lon2 = particle.lon + u2*.5*particle.dt\n",
    "        lat2 = particle.lat + v2*.5*particle.dt\n",
    "        dep2 = particle.depth + w2*.5*particle.dt/particle.fact\n",
    "        (u3, v3, w3) = fieldset.UVW[time + .5 * particle.dt, dep2, lat2, lon2]\n",
    "        lon3 = particle.lon + u3*particle.dt\n",
    "        lat3 = particle.lat + v3*particle.dt\n",
    "        dep3 = particle.depth + w3*particle.dt/particle.fact\n",
    "        (u4, v4, w4) = fieldset.UVW[time + particle.dt, dep3, lat3, lon3]\n",
    "        wa = (w1 + 2*w2 + 2*w3 + w4) /6.\n",
    "        particle.wa = wa* particle.dt\n",
    "        particle_dlon = (u1 + 2*u2 + 2*u3 + u4) / 6. * particle.dt\n",
    "        particle_dlat = (v1 + 2*v2 + 2*v3 + v4) / 6. * particle.dt\n",
    "        particle_ddepth = particle.wa/particle.fact + VVL\n",
    "        if particle_ddepth + particle.depth < 0:\n",
    "            particle_ddepth = - (particle_ddepth+particle.depth)\n",
    "    else:\n",
    "        particle_dlon = 0\n",
    "        particle_dlat = 0\n",
    "        particle_ddepth = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turb_mix(particle,fieldset,time):\n",
    "    #print('Turb_mix Kernel is Running')\n",
    "    if particle.status == 1 or particle.status == 2 or particle.status == 3:\n",
    "        \"\"\"Vertical mixing\"\"\"\n",
    "        #Vertical mixing\n",
    "        if particle.depth + 0.5/particle.fact > td: #Only calculate gradient of diffusion for particles deeper than 0.5 otherwise OP will check for particles outside the domain and remove it.\n",
    "            Kzdz = 2*(fieldset.vert_eddy_diff[time, particle.depth, particle.lat, particle.lon]-fieldset.vert_eddy_diff[time, particle.depth-0.5/particle.fact, particle.lat, particle.lon]) #backwards difference \n",
    "        else: \n",
    "            Kzdz = 2*(fieldset.vert_eddy_diff[time, particle.depth+0.5/particle.fact, particle.lat, particle.lon]-fieldset.vert_eddy_diff[time, particle.depth, particle.lat, particle.lon]) #forward difference \n",
    "        dgrad = Kzdz*particle.dt/particle.fact\n",
    "        if particle.depth+(0.5*dgrad) > 0 and particle.depth+(0.5*dgrad) < td:\n",
    "            Kz = fieldset.vert_eddy_diff[time, particle.depth+ 0.5*dgrad, particle.lat, particle.lon] #Vertical diffusivity SSC  \n",
    "        else:\n",
    "            Kz = 0#fieldset.vert_eddy_diff[time, particle.depth, particle.lat, particle.lon] \n",
    "\n",
    "        Rr = ParcelsRandom.uniform(-1, 1)\n",
    "        d_random = sqrt(3*2*Kz*particle.dt) * Rr/particle.fact\n",
    "        dzs = (dgrad + d_random)\n",
    "        particle.wm = dzs*particle.fact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea behind resuspenssion Kernel:\n",
    "\n",
    "- After particles are setteld at the bottom, reaching status 4, if bottom velocities reach a critical value, then PBDEs are resuspended in the form of Colloids (Status = 2), being scavanged away. A part is also resuspended in Particulate form, but since they sink really fast, then we can neglect it.\n",
    "\n",
    "- 'Question': How can we set it up? What do we need for this Kernel? (checked) :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resuspension(particle, fieldset, time):\n",
    "    if particle.status == 4:\n",
    "        threshold = 5 # threshold for particles to know when to resuspend\n",
    "        # Calculation of U_star, which is proportional to the bottom stress (tau)\n",
    "        k = 0.42\n",
    "        z_star = 0.07\n",
    "        u_horizontal = (1/4) * (fieldset.U[time, fieldset.mbathy - 1, particle.lat, particle.lon] + fieldset.U[time, fieldset.mbathy - 1, particle.lat, particle.lon -1]) ** 2\n",
    "        v_horizontal = (1/4) * (fieldset.V[time, fieldset.mbathy - 1, particle.lat, particle.lon] + fieldset.U[time, fieldset.mbathy - 1, particle.lat - 1, particle.lon]) ** 2\n",
    "        vel_horizontal = (u_horizontal + v_horizontal) ** (1/2)\n",
    " \n",
    "        u_star = (vel_horizontal * k) / (math.log(fieldset.e3t[time, fieldset.mbathy -1, particle.lat, particle.lon] / z_star))\n",
    "        # Here tau is the bottom friction parameter stimated from (u_starr)^2 x density\n",
    "        tau = ((u_star) ** 2) * 1024#(fieldset.sigma_theta[particle.time, fieldset.mbathy - 1, particle.lat, particle.lon] + 1000)\n",
    "        #\n",
    "        #############\n",
    "        frac_value = ParcelsRandom.randint(0,10)\n",
    "        if tau > threshold and frac_value >= 3:\n",
    "            particle.status = 3\n",
    "        #\n",
    "        if tau > threshold and frac_value < 3:\n",
    "            particle.status = 2\n",
    "            #\n",
    "            #particle.depth += sinkvel * particle.dt \n",
    "\n",
    "##### ADD HOW PARTICLES WILL RESUSPEND #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Displacement(particle,fieldset,time):\n",
    "    #print('Displacement Kernel is Running')\n",
    "    '''Apply movement calculated by other kernels'''\n",
    "    if particle.status == 1 or particle.status == 2 or particle.status == 3:\n",
    "        #Apply turbulent mixing.\n",
    "        if dzs + particle_ddepth + particle.depth > td:\n",
    "            particle.depth  = td # Get particles attached to the bottom when they reach it\n",
    "            particle.status = 4\n",
    "        #\n",
    "        elif dzs + particle.depth+ particle_ddepth < 0:\n",
    "            particle_ddepth = -(dzs + particle.depth+particle_ddepth) #reflection on surface\n",
    "        #\n",
    "        else:\n",
    "            particle_ddepth += dzs #apply mixing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export(particle,fieldset,time):\n",
    "    if particle.lat<48.7 and particle.lon < -124.66:\n",
    "        particle.status = 5\n",
    "\n",
    "def CheckOutOfBounds(particle, fieldset, time):\n",
    "    if particle.state == StatusCode.ErrorOutOfBounds:    \n",
    "        particle.delete()\n",
    "        \n",
    "def KeepInOcean(particle, fieldset, time):\n",
    "    if particle.state == StatusCode.ErrorThroughSurface:\n",
    "        particle.depth = 0.0\n",
    "        particle.state = StatusCode.Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pset_states.execute([states, process, PBDEs_forms, Advection, turb_mix, Displacement, resuspension, CheckOutOfBounds, export, KeepInOcean],\n",
    "            runtime=duration/2, \n",
    "            dt=dt,\n",
    "            output_file=pset_states.ParticleFile(name=outfile_states, outputdt=timedelta(hours=odt))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = xr.open_zarr(outfile_states)\n",
    "depth1 = ds1.z*ds1.fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the particles status. See if it's working\n",
    "fig, ax = plt.subplots(1,2,figsize=(10,4))\n",
    "i_p = 100\n",
    "i_t = 100\n",
    "ax[0].plot(ds1.status[:,i_t], '.k', label='status')\n",
    "ax[0].set_title('All Particles at time ' + str(i_t))\n",
    "ax[1].plot(ds1.status[i_p,:], '.k', label='status')\n",
    "ax[1].set_title('Particle ' + str(i_p) + ' out of ' + str(n) + ' at all times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(10,7))\n",
    "ax[0].plot(ds1.status, '.k', label='status')\n",
    "#\n",
    "ax[1].hist(ds1.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-States kernel is working ok; it's keeping the initial state for each particle through time.\n",
    "\n",
    "\n",
    "-Process Kernel it seems to be overwriting the States Kernel, letting particles to change just between status 2 and 3\n",
    "\n",
    "\n",
    "-PBDEs_forms Kernel is not affecting the other Kernels, since it's just changing a direct property of the particles, not changing their status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(2,2,figsize=(14,10))\n",
    "\n",
    "axs[0,0].scatter(ds1.lat[:,0],depth1[:,0],zorder=3,c='r',s=5)\n",
    "axs[0,0].scatter(clat[0],dmin, zorder=3, c='k', marker='*',s=30)\n",
    "axs[0,0].set_ylim([250,0])\n",
    "axs[0,0].set_title(f'Particles location at t = ' + str(ds1.time['obs'].values[0]))\n",
    "\n",
    "# Make map\n",
    "blevels = list(np.arange(0,500,100))\n",
    "\n",
    "im=axs[0,1].contourf(coords.nav_lon, coords.nav_lat, mask.totaldepth[:,:],zorder=1,cmap=cmap,levels=blevels)\n",
    "axs[0,1].contourf(coords.nav_lon, coords.nav_lat, mask.umask[0,0,:,:],zorder=2,cmap='gray',levels=[-1,0])\n",
    "axs[0,1].scatter(ds1.lon[:,0],ds1.lat[:,0],zorder=3,c='r',s=5)\n",
    "axs[0,1].scatter(clon[0],clat[0], zorder=3, c='k', marker='*',s=30)\n",
    "axs[0,1].set_ylim([clat[0]-.5,clat[0]+.5])\n",
    "axs[0,1].set_xlim([clon[0]-.5,clon[0]+.5])\n",
    "axs[0,1].set_title('Particles location at t = 0')\n",
    "\n",
    "im=axs[1,0].contourf(coords.nav_lon, coords.nav_lat, mask.totaldepth[:,:],zorder=1,cmap=cmap,levels=blevels)\n",
    "axs[1,0].contourf(coords.nav_lon, coords.nav_lat, mask.umask[0,0,:,:],zorder=2,cmap='gray',levels=[-1,0])\n",
    "axs[1,0].scatter(ds1.lon[:,-1],ds1.lat[:,-1],zorder=3,c='b',s=5)\n",
    "axs[1,0].scatter(clon[0],clat[0], zorder=3, c='k', marker='*',s=30)\n",
    "axs[1,0].set_ylim([clat[0]-.5,clat[0]+.5])\n",
    "axs[1,0].set_xlim([clon[0]-.5,clon[0]+.5])\n",
    "axs[1,0].set_title(f'Particles location at t = ' + str(ds1.time['obs'].values[-1]+1))\n",
    "\n",
    "axs[1,1].scatter(ds1.lat[:,-1],depth1[:,-1],zorder=3,c='b',s=5)\n",
    "axs[1,1].scatter(clat[0],dmin, zorder=3, c='k', marker='*',s=30)\n",
    "axs[1,1].set_ylim([250,0])\n",
    "axs[1,1].set_title(f'Particles location at t = ' + str(ds1.time['obs'].values[-1]+1))\n",
    "#\n",
    "cbar = fig.colorbar(im, ax=axs, location='right', shrink=0.8)\n",
    "cbar.set_label('Depth [m]')\n",
    "#\n",
    "plt.suptitle('Particles from Iona Outfall Depths ' + str(daterange[0]) + ' to ' + str(daterange[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1000\n",
    "#\n",
    "fig,axs=plt.subplots(2,2,figsize=(14,10))\n",
    "\n",
    "pp = axs[0,0].scatter(ds1.lat[:,0],depth1[:,0],zorder=3,c=ds1.status[:,0],s=5, cmap = 'jet')\n",
    "axs[0,0].scatter(clat[0],dmin, zorder=3, c='k', marker='*',s=30)\n",
    "axs[0,0].set_ylim([250,0])\n",
    "axs[0,0].set_title(f'Particles location at t = ' + str(ds1.time['obs'].values[0]))\n",
    "\n",
    "# Make map\n",
    "\n",
    "im=axs[0,1].contourf(coords.nav_lon, coords.nav_lat, mask.totaldepth[:,:],zorder=1,cmap=cmap,levels=blevels)\n",
    "axs[0,1].contourf(coords.nav_lon, coords.nav_lat, mask.umask[0,0,:,:],zorder=2,cmap='gray',levels=[-1,0])\n",
    "axs[0,1].scatter(ds1.lon[:,0],ds1.lat[:,0],zorder=3,c=ds1.status[:,0],s=5, cmap = 'jet')\n",
    "axs[0,1].scatter(clon[0],clat[0], zorder=3, c='k', marker='*',s=30)\n",
    "axs[0,1].set_ylim([clat[0]-.5,clat[0]+.5])\n",
    "axs[0,1].set_xlim([clon[0]-.5,clon[0]+.5])\n",
    "axs[0,1].set_title(f'Particles location at t = ' + str(ds1.time['obs'].values[0]))\n",
    "\n",
    "im=axs[1,0].contourf(coords.nav_lon, coords.nav_lat, mask.totaldepth[:,:],zorder=1,cmap=cmap,levels=blevels)\n",
    "axs[1,0].contourf(coords.nav_lon, coords.nav_lat, mask.umask[0,0,:,:],zorder=2,cmap='gray',levels=[-1,0])\n",
    "axs[1,0].scatter(ds1.lon[:,t],ds1.lat[:,t],zorder=3,c=ds1.status[:,t],s=5. , cmap = 'jet')\n",
    "axs[1,0].scatter(clon[0],clat[0], zorder=3, c='k', marker='*',s=30)\n",
    "axs[1,0].set_ylim([clat[0]-.5,clat[0]+.5])\n",
    "axs[1,0].set_xlim([clon[0]-.5,clon[0]+.5])\n",
    "axs[1,0].set_title(f'Particles location at t = ' + str(ds1.time['obs'].values[t]+1))\n",
    "\n",
    "particles = axs[1,1].scatter(ds1.lat[:,t],depth1[:,t],zorder=3,c=ds1.status[:,t],s=5, cmap = 'jet')\n",
    "axs[1,1].scatter(clat[0],dmin, zorder=3, c='k', marker='*',s=30)\n",
    "axs[1,1].set_ylim([250,0])\n",
    "axs[1,1].set_title(f'Particles location at t = ' + str(ds1.time['obs'].values[t]+1))\n",
    "#\n",
    "cbar = fig.colorbar(im, ax=axs, location='right', shrink=0.8)\n",
    "cbar.set_label('Depth [m]')\n",
    "#\n",
    "cbar1 = fig.colorbar(pp, ax=axs, location='right', shrink=0.8)\n",
    "cbar1.set_label('Status')\n",
    "#\n",
    "plt.suptitle('Particles from Iona Outfall Depths ' + str(daterange[0]) + ' to ' + str(daterange[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One particle plot / animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4 = np.where(ds1.status == 4)\n",
    "a3 = np.where(ds1.status == 3)\n",
    "a2 = np.where(ds1.status == 2)\n",
    "a1 = np.where(ds1.status == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_num = a1[0]\n",
    "fig, ax = plt.subplots(1,2,figsize = (12,6))\n",
    "#\n",
    "mapita=ax[0].contourf(coords.nav_lon, coords.nav_lat, mask.totaldepth[:,:],zorder=1,cmap=cmap,levels=blevels)\n",
    "ax[0].contourf(coords.nav_lon, coords.nav_lat, mask.umask[0,0,:,:],zorder=2,cmap='gray',levels=[-1,0])\n",
    "ax[0].scatter(ds1.lon[p_num,:],ds1.lat[p_num,:],zorder=3,c=ds1.status[p_num,:],s=5. , cmap = 'jet')\n",
    "ax[0].scatter(clon[0],clat[0], zorder=3, c='k', marker='*',s=30)\n",
    "ax[0].set_ylim([clat[0]-.5,clat[0]+.5])\n",
    "ax[0].set_xlim([clon[0]-.5,clon[0]+.5])\n",
    "#\n",
    "single = ax[1].scatter(ds1.lat[p_num,:],depth1[p_num,:],zorder=3,c=ds1.status[p_num,:],s=5, cmap = 'jet')\n",
    "ax[1].scatter(clat[0],dmin, zorder=3, c='k', marker='*',s=30)\n",
    "ax[1].set_ylim([250,0])\n",
    "#\n",
    "cbar = fig.colorbar(mapita, ax=ax[0], location='right', shrink=0.8)\n",
    "cbar.set_label('Depth [m]')\n",
    "#\n",
    "cbar1 = fig.colorbar(single, ax=ax[1], location='right', shrink=0.8)\n",
    "cbar1.set_label('Status')\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOTTOM MAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.where(ds1.status[:,-1] == 4)\n",
    "aa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = finder(ds1.lat[0,0].values, ds1.lon[0,0].values)\n",
    "print (\"The total depth at this location is\", mask.totaldepth[x, y].values, 'm', 'and the particle depth is ', ds1.z[0,0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(14,8))\n",
    "ax[0].contourf(coords.nav_lon, coords.nav_lat, mask.umask[0,0,:,:],zorder=2,cmap='gray',levels=[-1,0])\n",
    "particles = ax[0].scatter(ds1.lon[aa[0], -1], ds1.lat[aa[0], -1], s=5, c=ds1.z[aa[0], -1], cmap = cmap, vmin = 0, vmax = 170)\n",
    "ax[0].set_ylim([clat[0]-1.5,clat[0]+1.5])\n",
    "ax[0].set_xlim([clon[0]-2.0,clon[0]+.7])\n",
    "ax[0].scatter(clon[0],clat[0], zorder=3, c='r', marker='*',s=30)\n",
    "\n",
    "#ax[0].set_aspect(5/4.4)\n",
    "ax[0].set_title('Particles at the Bottom (Status = 4)')\n",
    "#\n",
    "#bathy=ax[1].contourf(coords.nav_lon, coords.nav_lat, mask.totaldepth[:,:],zorder=1,cmap=cmap)#,levels=blevels)\n",
    "ax[1].contourf(coords.nav_lon, coords.nav_lat, mask.umask[0,0,:,:],zorder=2,cmap='gray',levels=[-1,0])\n",
    "all_particles = ax[1].scatter(ds1.lon[:,t],ds1.lat[:,t],zorder=3,s=5, c=ds1.z[:, t], cmap = cmap, vmin = 0, vmax = 170)\n",
    "ax[1].scatter(clon[0],clat[0], zorder=3, c='r', marker='*',s=30)\n",
    "ax[1].set_ylim([clat[0]-1.5,clat[0]+1.5])\n",
    "ax[1].set_xlim([clon[0]-2.0,clon[0]+.7])\n",
    "ax[1].set_title(f'All Particles location at t = ' + str(ds1.time['obs'].values[t]+1))\n",
    "#\n",
    "cbar = fig.colorbar(particles, ax=ax[0], location='right', shrink=0.8)\n",
    "cbar.set_label('Particles Depth [m]')\n",
    "\n",
    "#\n",
    "cbar1 = fig.colorbar(all_particles, ax=ax[1], location='right', shrink=0.8)\n",
    "cbar1.set_label('Bottom Depth [m]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((len(ds1.z[aa[0],-1]) / len(ds1.z[:,-1])) *100, '% of Particles reach the bottom after ', length, ' days of simulation, between ' + str(daterange[0]) + ' and ' + str(daterange[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(19, 8))\n",
    "im = ax[0].contourf(coords.nav_lon, coords.nav_lat, mask.totaldepth[:,:],zorder=1,cmap=cmap,levels=blevels)\n",
    "ax[0].contourf(coords.nav_lon, coords.nav_lat, mask.umask[0,0,:,:],zorder=2,cmap='gray',levels=[-1,0])\n",
    "ax[0].scatter(ds1.lon[:, 0], ds1.lat[:, 0],s=2, c='k', cmap = 'jet')\n",
    "ax[1].scatter(clat[0],dmin, zorder=3, c='k', marker='*',s=30)\n",
    "ax[0].scatter(clon[0],clat[0], zorder=3, c='k', marker='*',s=30)\n",
    "#ax.legend()\n",
    "ax[0].set_ylim([clat[0]-.5,clat[0]+.5])\n",
    "ax[0].set_xlim([clon[0]-.5,clon[0]+.5])\n",
    "ax[0].set_aspect(5/4.4)\n",
    "ax[1].set_ylim([250,0])\n",
    "ax[0].set_xlabel('Longitude')\n",
    "ax[0].set_ylabel('Latitude')\n",
    "ax[0].set_title(str(daterange[0]) + ' to ' + str(daterange[-1]))\n",
    "nmin, nmax = 0, -1\n",
    "#\n",
    "cbar = fig.colorbar(im, ax=ax[0], location='right', shrink=0.8)\n",
    "cbar.set_label('Depth [m]')\n",
    "#\n",
    "FFMpegWriter = animation.writers['ffmpeg']  \n",
    "metadata = dict(title='Animation_Particles', artist='Vicente',\n",
    "                comment='Particles movement (Year 2022)')\n",
    "writer = FFMpegWriter(fps=12, metadata=metadata)\n",
    "\n",
    "with writer.saving(fig, path['anim']+\"/Year_run_PBDEs_no_POC.mp4\", 100):\n",
    "    for tm in range(len(ds1.obs)):\n",
    "        s = ax[0].scatter(ds1.lon[:, nmin:tm], ds1.lat[:, nmin:tm],s=2, c=ds1.status[:,nmin:tm], cmap = 'jet')\n",
    "        a = ax[1].scatter(ds1.lat[:,nmin:tm],depth1[:,nmin:tm],zorder=3,c=ds1.status[:,nmin:tm],s=5, cmap = 'jet')\n",
    "        if tm == 0:  # Adding colorbar only once\n",
    "            a.set_clim(ds1.status.min().values, ds1.status.max().values)\n",
    "            cb1 = fig.colorbar(a, ax=ax[1])\n",
    "            cb1.set_label('Status')\n",
    "        ax[1].set_title(f'Particles location at t = ' + str(ds1.time['obs'].values[tm]+1))\n",
    "        writer.grab_frame()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Parcels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
